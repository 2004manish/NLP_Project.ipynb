{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "These are ISO 639-1 language codes representing different languages. Here's their meaning:\n",
        "\n",
        "\n",
        "- **en** : English\n",
        "- **ru** : Russian\n",
        "- **es** : Spanish\n",
        "- **no** : Norwegian\n",
        "- **hi** : Hindi\n",
        "- **ja** : Japanese\n",
        "- **zh-CN** : Simplified Chinese (as used in mainland China)\n",
        "\n",
        "These codes are widely used in localization, software, and linguistics to identify languages."
      ],
      "metadata": {
        "id": "ginStFf98Mwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing the Library"
      ],
      "metadata": {
        "id": "UjdsWkE86HmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Using langdetect library This module is a port of Google’s language-detection library that supports 55 languages."
      ],
      "metadata": {
        "id": "d82EOurs6K6j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQSUUobX5sHa",
        "outputId": "62524d76-002b-40b9-8b24-06bedd70f33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/981.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=47dcc1975e3181f86184970b249b61c421c28e708bad2809b6d12ebe687bb4d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to demonstrate langdetect\n",
        "\n",
        "\n",
        "from langdetect import detect\n",
        "\n",
        "\n",
        "# Specifying the language for detection\n",
        "\n",
        "print(detect(\"AI refers to the systems that can perform tasks automatically.\"))\n",
        "print(detect(\"Искусственный интеллект относится к системам, которые могут выполнять задачи автоматически.\"))\n",
        "print(detect(\"La IA se refiere a los sistemas que pueden realizar tareas automáticamente.\"))\n",
        "print(detect(\"AI refererer til systemene som kan utføre oppgaver automatisk.\"))\n",
        "print(detect(\"एआई उन सिस्टमों को संदर्भित करता है जो स्वचालित रूप से कार्य कर सकते हैं।\"))\n",
        "print(detect(\"AIは自動的にタスクを実行できるシステムを指します。\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8AIMUJ5yt-",
        "outputId": "040b37a2-afef-4c02-eeed-64697b959ee8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n",
            "ru\n",
            "es\n",
            "no\n",
            "hi\n",
            "ja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "en --- english\n",
        "\n",
        "ru --- russian\n",
        "\n",
        "es --- spanish\n",
        "\n",
        "no --- Norwegian\n",
        "\n",
        "hi --- hindi\n",
        "\n",
        "ja --- japanese\n",
        "\n"
      ],
      "metadata": {
        "id": "_ltBNhqo8QwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Using **textblob library** This module is used for natural language processing(NLP) tasks such as noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "(‘ru’, -641.3409600257874)"
      ],
      "metadata": {
        "id": "e8YK9JKC6f9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKEXKndA6XsZ",
        "outputId": "f636b8f5-fc95-4acb-ff82-35b88c7d489f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLyIl0Mz6uXN",
        "outputId": "3f3f7499-3b69-4ea4-c3d5-1716e03b737d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.11.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.11.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=7110e7ec9841526987c982b24bcdc5a9bf1ecd6063de1147768722148c162b43\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.6\n",
            "    Uninstalling httpcore-1.0.6:\n",
            "      Successfully uninstalled httpcore-1.0.6\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.2\n",
            "    Uninstalling httpx-0.27.2:\n",
            "      Successfully uninstalled httpx-0.27.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.1.143 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.54.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "\n",
        "L = [\"AI refers to the systems that can perform tasks automatically.\",\n",
        "\t\"Искусственный интеллект относится к системам, которые могут выполнять задачи автоматически.\",\n",
        "\t\"La IA se refiere a los sistemas que pueden realizar tareas automáticamente.\",\n",
        "\t\"AI refererer til systemene som kan utføre oppgaver automatisk.\",\n",
        "\t\"एआई उन सिस्टमों को संदर्भित करता है जो स्वचालित रूप से कार्य कर सकते हैं।\",\n",
        "\t\"AIは自動的にタスクを実行できるシステムを指します。\",\n",
        "\t]\n",
        "\n",
        "translator = Translator() # Create a Translator object\n",
        "\n",
        "for i in L:\n",
        "\t# Language Detection using Google Translate API\n",
        "\tdetected = translator.detect(i)\n",
        "\tprint(detected.lang)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMBhke886zqw",
        "outputId": "71866520-2d85-4c66-e801-11141c115c3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n",
            "ru\n",
            "es\n",
            "no\n",
            "hi\n",
            "ja\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "\n",
        "word = input(\"Enter any word or sentence of any Language: \")\n",
        "print(word)\n",
        "\n",
        "translator = Translator() # Create a Translator object\n",
        "\n",
        "# Language Detection using Google Translate API\n",
        "detected = translator.detect(word)\n",
        "print(detected.lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf_VPgd_VPWu",
        "outputId": "89411f4b-39f7-4b54-e79c-76724fec0895"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any word or sentence of any Language: abcd\n",
            "abcd\n",
            "en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from googletrans import Translator\n",
        "\n",
        "word = input(\"Enter any word or sentence of any Language: \")\n",
        "print(word)\n",
        "\n",
        "translator = Translator() # Create a Translator object\n",
        "\n",
        "# Language Detection using Google Translate API\n",
        "detected = translator.detect(word)\n",
        "print(detected.lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdrl8x1DVd_P",
        "outputId": "cbdb91be-babe-442c-a032-a79df84fae94"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any word or sentence of any Language: 自動\n",
            "自動\n",
            "zh-TW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Using **langid library** This module is a standalone Language Identification tool. It is pre-trained over a large number of languages (currently 97). It is a single.py file with minimal dependencies."
      ],
      "metadata": {
        "id": "Jze2y7lk7hZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F7ppYit7bzT",
        "outputId": "e8dbf0e3-b6d0-436c-f628-20f5fb08383a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.26.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=d59fea4ca648709ec54fc6f58c80f46097514176dd9a49141f8ab13eda6694e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python program to demonstrate\n",
        "# langid\n",
        "\n",
        "\n",
        "import langid\n",
        "\n",
        "\n",
        "L = [\"AI refers to the systems that can perform tasks automatically.\",\n",
        "\t\"Искусственный интеллект относится к системам, которые могут выполнять задачи автоматически.\",\n",
        "\t\"La IA se refiere a los sistemas que pueden realizar tareas automáticamente.\",\n",
        "\t\"AI refererer til systemene som kan utføre oppgaver automatisk.\",\n",
        "\t\"एआई उन सिस्टमों को संदर्भित करता है जो स्वचालित रूप से कार्य कर सकते हैं।\",\n",
        "\t\"AIは自動的にタスクを実行できるシステムを指します。\",\n",
        "\t]\n",
        "\n",
        "for i in L:\n",
        "\n",
        "    # Language detection\n",
        "    print(langid.classify(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEiUl87T7xzG",
        "outputId": "fd988ba2-4c13-4aeb-ab48-d05d2e0512bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('en', -149.66131234169006)\n",
            "('ru', -1781.6450893878937)\n",
            "('es', -228.5087342262268)\n",
            "('nb', -103.85199356079102)\n",
            "('hi', -426.2300704717636)\n",
            "('ja', -635.0262796878815)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scores (Log-Probabilities):\n",
        "\n",
        "The numbers (e.g., -119.93 for en, -641.34 for ru) are log-probabilities. They indicate how likely the input text is to be in the corresponding language.\n",
        "\n",
        "Higher (less negative) scores mean higher confidence.\n",
        "\n",
        "The language with the highest score (closest to 0) is the most likely match.\n",
        "\n",
        "\n",
        "Example Interpretation:\n",
        "\n",
        "For the input you tested:\n",
        "\n",
        "en: -119.93\n",
        "\n",
        "ru: -641.34\n",
        "\n",
        "es: -191.01\n",
        "\n",
        "zh: -199.18\n",
        "\n",
        "hi: -286.99\n",
        "\n",
        "ja: -875.66\n",
        "\n",
        "The Langid tool would classify the input as English (en) because it has the highest score (-119.93). Lower scores (e.g., -875.66 for ja) indicate much lower confidence for those languages."
      ],
      "metadata": {
        "id": "ZSMP7kcY-fYK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6dc73G173Ab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}